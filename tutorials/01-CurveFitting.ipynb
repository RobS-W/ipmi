{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDdbPOAglmA2"
   },
   "source": [
    "# 1 Linear Regression - Curve Fitting (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pq93TYwjlmA8"
   },
   "source": [
    "## 1.1 Data generation\n",
    "First, we need to generate the ground truth data. We begin by defining the following polynomial to be the \"true\" model: \n",
    "\n",
    "\\begin{equation}\n",
    "t = w_2x^2 + w_1x +w_0\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "To generate the target values $t$, we sample $n$ equidistant values for $x$ in a range of $[-1,1]$ and use the above polynomial equation to generate $t$. To \"simulate\" the real-world scenario, where the observed data usually come with stochastic noise, we add random Gaussian noise to our target values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNL18VKdlmBB"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# get ground-truth data from the \"true\" model \n",
    "n = 20  # number of data samples\n",
    "# when generating our variables, we will make use of list comprehension in Python\n",
    "x = [(idx-round(n/2))/(n/2) for idx in range(n)]\n",
    "print(x)\n",
    "\n",
    "w = [1, 2, 3]\n",
    "t = [xn**2*w[2] + xn*w[1] + w[0] for xn in x]\n",
    "print(t)\n",
    "\n",
    "# \"simulated\" observed target values\n",
    "std_noise = 0.2\n",
    "t_observed = [t[idx]+random.gauss(0,std_noise) for idx in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python plotting library [matplotlib](https://matplotlib.org/) is used to plot the observed target values `t_observed` with respect to each sampling location $x$ in blue points [`x`,`t_observed`]. We can also plot the noise-free \"true\" target values `t` versus the same `x`. This represents the underlying model as a curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-vN0zD8lmBa"
   },
   "outputs": [],
   "source": [
    "# this line is useful for jupyter notebook only\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the curve and the noise-corrupted data\n",
    "plt.plot(x,t,'r')\n",
    "plt.plot(x,t_observed,'bo')\n",
    "plt.ylabel('$t$')\n",
    "plt.xlabel('$x$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XyAazZJKlmBx"
   },
   "source": [
    "## 1.2 Model Fitting\n",
    "For numerical computations such as curve fitting, we can use Python's powerful [numpy](http://www.numpy.org/) library. In this tutorial, we use [*numpy.linalg.lstsq*](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq) to solve [a system of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations) $\\textbf{a}\\textbf{x}=\\textbf{b}$ for a [least sqaures solution](https://en.wikipedia.org/wiki/Least_squares).\n",
    "\n",
    "Re-arrange the observed data to a linear system in matrix form \"$\\textbf{a}\\textbf{x}=\\textbf{b}$\" (N.B. in this form $\\textbf{x}$ is the unknown, not our learning input $x$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{t} = \\textbf{X}\\textbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "that is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{vmatrix}\n",
    "t_{(1)} \\\\ t_{(2)} \\\\ \\cdots \\\\ t_{(n)}\n",
    "\\end{vmatrix} = \\begin{vmatrix}\n",
    "x^2_{(1)} & x_{(1)} & 1 \\\\\n",
    "x^2_{(2)} & x_{(2)} & 1 \\\\\n",
    "& \\cdots \\\\\n",
    "x^2_{(n)} & x_{(n)} & 1 \n",
    "\\end{vmatrix} \\cdot\n",
    "\\begin{vmatrix}\n",
    "w_2 \\\\ w_1 \\\\ w_0\n",
    "\\end{vmatrix}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kLuMm_vlmB0"
   },
   "outputs": [],
   "source": [
    "# use nunmpy for a least-square solution to the linear system \"Xw=t\"\n",
    "import numpy as np\n",
    "\n",
    "# first we need to turn t and x into numpy arrays as column vectors \n",
    "t_observed = np.reshape(t_observed, [-1, 1])\n",
    "x_1 = np.reshape(x, [-1, 1])\n",
    "x_2 = np.square(x_1)\n",
    "x_0 = np.ones_like(x_1)\n",
    "X = np.concatenate([x_2,x_1,x_0],1)\n",
    "# print to check the inputs and their array shapes\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(t_observed.shape)\n",
    "w_estimate = np.linalg.lstsq(X, t_observed, rcond=None)\n",
    "print(w_estimate[0])  # print the output\n",
    "\n",
    "# calculate the estimated curve, i.e.\n",
    "# t_estimate = [xn**2*w_estimate[2]+xn*w_estimate[1]+w_estimate[0] for xn in x]\n",
    "# but matrix multiplication is more compact:\n",
    "t_estimate = np.matmul(X,w_estimate[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`w_estimate` are the best estimates of the weights $w$ - in a least squares sense. Finally, we can add a green curve (formed by points [`x`, `t_estimate`]) to the previous plot to visualise the estimated polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kLuMm_vlmB0"
   },
   "outputs": [],
   "source": [
    "plt.plot(x,t,'r', label='Ground truth')\n",
    "plt.plot(x,t_observed,'bo', label = 'Observed data')\n",
    "plt.plot(x,t_estimate,'g', label='Estimated  curve fit')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$t$')\n",
    "leg = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zaoUWBtlmCC"
   },
   "source": [
    "## 1.3 Model Fitting Error\n",
    "From the above plot, we can see that the green curve (i.e. the fitted polynomial model) does not go through all of the observed points (in blue) which were used in fitting the model. This is known as redidual error (also known as training error). \n",
    "\n",
    "There is also discrepancy between the \"true\" curve (in red) and the estimated one (in green), but this difference is often impossible to obtain without access to the true underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxRPpS-s_EPP"
   },
   "outputs": [],
   "source": [
    "# Calculate the sum of residuals and the root-mean-square error\n",
    "\n",
    "# residuals:\n",
    "Residuals = t_estimate-t_observed\n",
    "SR = np.sum(np.square(Residuals))  # sums of residuals: b - a*x\n",
    "# root-mean-square error\n",
    "RMSE = np.sqrt(np.mean(np.square(Residuals)))\n",
    "print(SR)\n",
    "print(RMSE)\n",
    "\n",
    "# plot the error distribution\n",
    "plt.hist(Residuals)\n",
    "plt.xlabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUlz_pjSSwv9"
   },
   "source": [
    "## Questions\n",
    "### Sample size\n",
    "1. What is the effect on changing the sample size `n` on the model fitting and its errors?\n",
    "2. How many samples are needed and why? \n",
    "3. What is the effect on the required sample size when the number of parameters in `w` is increased (i.e. increasing the order of the true polynomial)? \n",
    "\n",
    "### Model fitting\n",
    "1. What is the objective (loss) function in this curve fitting problem?\n",
    "2. How does change of the noise level (i.e. `std_noise`) affect the model fitting?\n",
    "3. Are residualds a good estimate of the difference to the \"true\" target values and why?\n",
    "4. How can we measure how \"good\" (or generalisable) the `t_estimated` values are?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tutorials_01-LinearRegression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
