{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDdbPOAglmA2"
   },
   "source": [
    "# 1 Linear Regression - curve fitting (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pq93TYwjlmA8"
   },
   "source": [
    "## 1.1 Data generation\n",
    "\n",
    "\\begin{equation}\n",
    "t = w_2x^2 + w_1x +w_0\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "This is the \"true\" model for the curve.  \n",
    "First we sample $n$ equidistant values for $x$, approxaimtely in the range of $[0,1]$; Then compute $n$ corresponding target values for $t$ using the above polynomial equation; To \"simulate\" the real-world scenario where the observed data usually come with noise, add random Gaussian noise to our target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNL18VKdlmBB"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# get ground-truth data from the \"true\" model \n",
    "n = 20  # number of data samples\n",
    "x = [(idx-round(n/2))/(n/2) for idx in range(n)]\n",
    "print(x)\n",
    "\n",
    "w = [1, 2, 3]\n",
    "t = [xn**2*w[2] + xn*w[1] + w[0] for xn in x]\n",
    "print(t)\n",
    "\n",
    "# \"simulated\" observed target values\n",
    "std_noise = 0.2\n",
    "t_observed = [t[idx]+random.gauss(0,std_noise) for idx in range(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use python plotting library [matplotlib](https://matplotlib.org/) to plot the red curve (directly from the underlying model) and the observed blue data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-vN0zD8lmBa"
   },
   "outputs": [],
   "source": [
    "# this is useful for jupyter notebook only\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the curve and the noise-corrupted data\n",
    "plt.plot(x,t,'r')\n",
    "plt.plot(x,t_observed,'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XyAazZJKlmBx"
   },
   "source": [
    "## 1.2 Model Fitting\n",
    "For numerical computations such as curve fitting, we can use the powerful [numpy](http://www.numpy.org/). In this tutorial, we use [*numpy.linalg.lstsq*](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq) to solve [a system of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations) $\\textbf{a}\\textbf{x}=\\textbf{b}$ for a [least sqaures solution](https://en.wikipedia.org/wiki/Least_squares).\n",
    "\n",
    "Re-arrange to a linear system in matrix form $\\textbf{a}\\textbf{x}=\\textbf{b}$ (N.B. the x is the unknown here, not the learning input x):\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{t} = \\textbf{X}\\textbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "that is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{vmatrix}\n",
    "t_{(1)} \\\\ t_{(2)} \\\\ \\cdots \\\\ t_{(n)}\n",
    "\\end{vmatrix} = \\begin{vmatrix}\n",
    "x^2_{(1)} & x_{(1)} & 1 \\\\\n",
    "x^2_{(2)} & x_{(2)} & 1 \\\\\n",
    "& \\cdots \\\\\n",
    "x^2_{(n)} & x_{(n)} & 1 \n",
    "\\end{vmatrix} \\cdot\n",
    "\\begin{vmatrix}\n",
    "w_2 \\\\ w_1 \\\\ w_0\n",
    "\\end{vmatrix}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kLuMm_vlmB0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# then use nunmpy for a least-square solution to the linear system \"Xw=t\"\n",
    "t_observed = np.reshape(t_observed, [-1, 1])\n",
    "x_1 = np.reshape(x, [-1, 1])\n",
    "x_2 = np.square(x_1)\n",
    "x_0 = np.ones_like(x_1)\n",
    "X = np.concatenate([x_2,x_1,x_0],1)\n",
    "# print to check the inputs\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(t_observed.shape)\n",
    "w_estimate = np.linalg.lstsq(X, t_observed, rcond=None)\n",
    "print(w_estimate[0])  # print the output\n",
    "\n",
    "# plot to see the estimated curve, i.e.\n",
    "# t_estimate = [xn**2*w_estimate[2]+xn*w_estimate[1]+w_estimate[0] for xn in x]\n",
    "# but matrix multiplication is more compact:\n",
    "t_estimate = np.matmul(X,w_estimate[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*w_estimate* should be the best estimate of the weights $w$.  \n",
    "Finally, we can add the green curve to the previous plot to visualise the estimated polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kLuMm_vlmB0"
   },
   "outputs": [],
   "source": [
    "plt.plot(x,t,'r')\n",
    "plt.plot(x,t_observed,'bo')\n",
    "plt.plot(x,t_estimate,'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zaoUWBtlmCC"
   },
   "source": [
    "## 1.3 Model Fitting Error\n",
    "From the above plot, we can see that the green curve (i.e. the estimated polynomial model) does not go through all the observed points (in blue) which were used in fitting the model. This is also known as training error. There is also discrepancy between underlying curve (in red) and the estimated one (in green), but this difference may not be possible to calculate without access to the true underlying model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxRPpS-s_EPP"
   },
   "outputs": [],
   "source": [
    "# residuals:\n",
    "Residuals = t_estimate-t_observed\n",
    "SR = np.sum(np.square(Residuals))  # sums of residuals: b - a*x\n",
    "# root-mean-square error\n",
    "RMSE = np.sqrt(np.mean(np.square(Residuals)))\n",
    "print(SR)\n",
    "print(RMSE)\n",
    "\n",
    "# plot the error distribution\n",
    "plt.hist(Residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUlz_pjSSwv9"
   },
   "source": [
    "## Questions\n",
    "### Sample size\n",
    "- Effect on changing the sample size on model fitting and its errors.\n",
    "- How many samples are needed?\n",
    "\n",
    "### Model fitting\n",
    "- What is the objective (loss) function?\n",
    "- What is the difference to the true target values?\n",
    "- How to measure goodness-of-fit?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tutorials_01-LinearRegression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
